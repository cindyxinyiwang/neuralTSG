### vocabulary size ###
# source vocabulary size: [1, +00)
[source vocabulary size] 15
# target vocabulary size: [1, +00)
[target vocabulary size] 15

### network architecture ###
# source word embedding dimension: [1, +00)
[source word embedding dimension] 620
# target word embedding dimension: [1, +00)
[target word embedding dimension] 620
# encoder hidden layer dimension: [1, +00)
[encoder hidden layer dimension] 1000
# decoder hidden layer dimension: [1, +00)
[decoder hidden layer dimension] 1000

### training ###
## minimum risk training (MRT) setting 
# number of sentences to be sampled: [1, +00).
[MRT sample size] 100
# length ratio limit of sampled sentences: (0, +00)
[MRT length ratio limit] 1.5

### training ###
# maximum sentence length: [1, +00)
[maximum sentence length] 50
# number of sentences in a mini-bath: [1, +00).
[mini-batch size] 2
# number of mini-batches to be sorted: [1, +00) 
[mini-batch sorting size] 20
# maximum iteration in training: [1, +00)
[iteration limit] 1000000
# number of iterations with decreasing BLEUs before convergence: [1, +00)
[convergence limit] 100000

### optimization ###
# optimizer: 0 for SGD, 1 for AdaDelta, 2 for Adam with slow start, 3 for Adam
[optimizer] 2
# clipping
[clip] 1.0

# SGD setting
[SGD learning rate] 1.0

# AdaDelta setting
[AdaDelta rho] 0.95
[AdaDelta epsilon] 1e-6

# Adam setting
[Adam alpha] 0.0005
[Adam alpha decay] 0.998
[Adam beta1] 0.9
[Adam beta2] 0.999
[Adam eps] 1e-8

### search ###
[beam size] 10

### model dumping ###
# iteration for dumping and validating intermediate models
[model dumping iteration] 2000

### interrupted training restoring ###
[checkpoint iteration] 2000
